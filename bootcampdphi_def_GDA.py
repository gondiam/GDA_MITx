# -*- coding: utf-8 -*-
"""bootcampDPhi_def.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oBwIIx_OujDXXlj8vzQQ727KKsahz3wU
"""

#https://dphi.tech/notebooks/1952

!pip install opencv-python

import pandas as pd # Data analysis and manipultion tool
import numpy as np # Fundamental package for linear algebra and multidimensional arrays
import tensorflow as tf # Deep Learning Tool
import os # OS module in Python provides a way of using operating system dependent functionality
import cv2 # Library for image processing
from sklearn.model_selection import train_test_split # For splitting the data into train and validation set
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras import regularizers, optimizers

from google_drive_downloader import GoogleDriveDownloader as gdd

gdd.download_file_from_google_drive(file_id='1f7uslI-ZHidriQFZR966_aILjlkgDN76',
dest_path='content/eye_gender_data.zip',
unzip=True)

labels = pd.read_csv("/content/content/eye_gender_data/Training_set.csv") # loading the labels
file_paths = [[fname, '/content/content/eye_gender_data/train/' + fname] for fname in labels['filename']]
images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])
train_data = pd.merge(images, labels, how = 'inner', on = 'filename')

data = [] # initialize an empty numpy array
#TAMAÑO
image_size = 32 # image size taken is 100 here. one can take other size too
for i in range(len(train_data)):
  img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE) # converting the image to gray scale

  new_img_array = cv2.resize(img_array, (image_size, image_size)) # resizing the image array
  data.append(new_img_array)

df = labels

import os
train_data_dir = '/content/content/eye_gender_data/train/'  
test_data_dir = '/content/content/eye_gender_data/test/'
target_names = df['label']
nb_train_samples = sum([len(files) for _, _, files in os.walk(train_data_dir)])  
nb_test_samples = sum([len(files) for _, _, files in os.walk(test_data_dir)])
total_nb_samples = nb_train_samples + nb_test_samples
# number of epochs to train top model 
epochs = 7 #this has been changed after multiple model run 
# batch size used by flow_from_directory and predict_generator 
batch_size = total_nb_samples/nb_test_samples

nb_classes = target_names.unique()      # number of output classes

print('Training a CNN Multi-Classifier Model ......')
print('\n - names of classes: ', target_names, '\n - # of classes: ', nb_classes)
print(' - # of trained samples: ', nb_train_samples, 
      '\n - # of test samples: ', nb_test_samples,
       '\n - total # of samples: ', total_nb_samples, '\n - train ratio:', round(nb_train_samples/total_nb_samples*100, 2),
      '\n - test ratio:', round(nb_test_samples/total_nb_samples*100, 2),
     ' %', '\n - # of epochs: ', epochs, '\n - batch size: ', batch_size)

import keras
train_v = []
import glob
#Datos de entrenamiento subidos
#https://stackoverflow.com/questions/52851866/how-to-deal-with-thousands-of-images-for-cnn-training-keras
import re
img_dir = '/content/content/eye_gender_data' # Enter Directory of all images 
image_path = os.path.join(img_dir+"/train",'*.jpg')
files = glob.glob(image_path)
images = []
masks = []
contours = []
indexes = []
files_names = []

for f1 in np.sort(files):
  img = cv2.imread(f1)
  result = re.search('Image_(.*).jpg', str(f1))
  idx = result.group(1)
  mask_path = img_dir+"/masks/mask_cropped_"+str(idx)+".jpg"
  mask = cv2.imread(mask_path,0)
  contour_path = img_dir+"/contours/contour_cropped_"+str(idx)+".jpg"
  contour = cv2.imread(contour_path,0)

  indexes.append(idx)
  images.append(img)
  masks.append(mask)
  contours.append(contour)

#Empieza con esto mejor:
#https://stackoverflow.com/questions/59464409/loading-images-in-keras-for-cnn-from-directory-but-label-in-csv-file
#Tengo el identificador y el label en el csv que es el df['label']
#Igual es más fácil que lo de arriba...espero
#x_col="Image"
#y_col="Id"

x_col = "filename"
y_col = "label"
batch_1=5

testdf =  pd.read_csv("/content/content/eye_gender_data/Testing_set.csv")

#batch_size:Set this to some number that divides your total number of images in your test set exactly.
traindf = df
datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)
train_generator=datagen.flow_from_dataframe(
dataframe=traindf,
directory=train_data_dir,
x_col="filename",
y_col="label",
subset="training",
batch_size=batch_1,
seed=42,
shuffle=True,
class_mode="categorical",
target_size=(image_size,image_size))

test_datagen=ImageDataGenerator(rescale=1./255.)
test_generator=test_datagen.flow_from_dataframe(
dataframe=testdf,
directory=test_data_dir,
x_col="filename",
y_col=None,
batch_size=batch_1,
seed=42,
shuffle=False,
class_mode=None,
target_size=(image_size,image_size))

valid_generator=datagen.flow_from_dataframe(
dataframe=traindf,
directory=train_data_dir,
x_col="filename",
y_col="label",
subset="validation",
batch_size=batch_1,
seed=42,
shuffle=True,
class_mode="categorical",
target_size=(image_size,image_size))

model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
model.compile(optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),loss="categorical_crossentropy",metrics=["accuracy"])

#Model
model = Sequential()
model.add(Conv2D(image_size, (3, 3), padding='same', input_shape=(image_size,image_size,3)))
model.add(Activation('relu'))
model.add(Conv2D(image_size, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(image_size*2, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(image_size*2, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(image_size*16))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
#model.compile(optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),loss="binary_crossentropy",metrics=["accuracy"])

from keras.layers import BatchNormalization,Conv2D,MaxPool2D,Dense,Flatten,Input,GlobalMaxPooling2D,Dropout
from keras.models import Sequential
from keras.callbacks import EarlyStopping,ReduceLROnPlateau
from sklearn.metrics import classification_report
from keras.preprocessing.image import ImageDataGenerator
from keras.constraints import maxnorm

model=Sequential()
#model.add(Input(shape=(image_size,image_size,3)))
model.add(Conv2D(image_size*2,(3,3),padding='same',activation='relu'))
model.add(MaxPool2D(2,2))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Conv2D(image_size*2,(3,3),padding='same',activation='relu'))
model.add(MaxPool2D(2,2))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Conv2D(image_size/2,(3,3),padding='same',activation='relu'))
model.add(MaxPool2D(2,2))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(image_size*4,activation='relu'))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# 1st conv block
model=Sequential()
#model.add(Input(shape=(image_size,image_size,3)))
model.add(Conv2D(image_size,(3,3),padding='same',activation='relu'))
#model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
# 2nd conv block
model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
model.add(BatchNormalization())
# 3rd conv block
model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))
model.add(BatchNormalization())
# ANN block
model.add(Flatten())
model.add(Dense(units=400, activation='relu'))
model.add(Dense(units=100, activation='relu'))
model.add(Dense(units=50, activation='relu'))
model.add(Dense(units=10, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
# output layer
model.add(Dense(units=2, activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

#Fitting the model
STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size
STEP_SIZE_TEST=test_generator.n//test_generator.batch_size


model.fit_generator(generator=train_generator,steps_per_epoch=STEP_SIZE_TRAIN,validation_data=valid_generator,validation_steps=STEP_SIZE_VALID,epochs=10)

#Evaluate the model
model.evaluate_generator(generator=valid_generator,steps=STEP_SIZE_TEST)

#Prediction
test_generator.reset()
pred=model.predict_generator(test_generator,
steps=STEP_SIZE_TEST,
verbose=1)

predicted_class_indices=np.argmax(pred,axis=1)



labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

filenames=test_generator.filenames
results=pd.DataFrame({ "Filename":filenames,"label":predictions})
results = results[['label']]
print(results)
# get table
values, counts = np.unique(results, return_counts=True)
print(values, counts/results.shape[0])
values, counts = np.unique(df["label"], return_counts=True)
print(values, counts/df.shape[0])

results.to_csv('results.csv')
from google.colab import files
files.download("results.csv")